# Tokyo-2021-olympics-data-engineering-project
Tokyo 2021 Olympics End to End data engineering project - Azure/Databricks

# ğŸ… Tokyo Olympics Data Engineering Project

This project builds an end-to-end data pipeline using Snowflake to ingest, transform, and analyze Tokyo 2020 Olympics data. It includes multiple CSV data sources covering athletes, coaches, teams, gender entries, and medals.

---

## ğŸ“¦ Data Sources

The following datasets were used:
- `Athletes.csv` â€“ Athlete-level data including name, discipline, and team
- `Coaches.csv` â€“ Coach information by team and discipline
- `EntriesGender.csv` â€“ Gender-wise athlete entries per discipline and team
- `Medals.csv` â€“ Medal winners by event, team, and medal type
- `Teams.csv` â€“ Team and country mappings

---

## ğŸ§± Tech Stack

| Layer         | Tool/Service          |
|---------------|------------------------|
| Data Warehouse | Snowflake (Standard Edition) |
| Ingestion     | Python + Snowflake Connector |
| Transformation| dbt Core               |
| Visualization | Metabase / Looker Studio / Power BI |

---

## âš™ï¸ Pipeline Steps

1. **Ingestion**  
   Load all CSV files into Snowflake `raw_` staging tables using Python.

2. **Transformation**  
   Use dbt to clean and model the data into star schema:
   - `dim_athletes`
   - `dim_teams`
   - `fact_medals`
   - `agg_gender_entries`

3. **Visualization**  
   Explore and present insights through dashboards:
   - Medal count by country
   - Gender participation trends
   - Top-performing athletes and disciplines

---

## ğŸ“Š Sample Insights

- Which countries won the most gold medals?
- How does gender participation differ across sports?
- What are the most medal-rich disciplines?

---

## ğŸ“ Folder Structure
â”œâ”€â”€ ingestion/
â”‚ â””â”€â”€ load_csv_to_snowflake.py
â”œâ”€â”€ dbt_project/
â”‚ â””â”€â”€ models/
â”‚ â””â”€â”€ staging/
â”‚ â””â”€â”€ marts/
â”œâ”€â”€ dashboards/
â”‚ â””â”€â”€ screenshots/
â”œâ”€â”€ data/
â”‚ â””â”€â”€ Athletes.csv
â”‚ â””â”€â”€ Medals.csv
â”‚ â””â”€â”€ ...
â”œâ”€â”€ README.md


---

## âœ… How to Reproduce

1. Clone the repo
2. Set up Snowflake credentials in a `.env` file
3. Run the ingestion script
4. Initialize and run `dbt`
5. Connect a BI tool to your Snowflake DB for visualization

---

## ğŸš€ Future Work

- Add incremental load support
- Schedule dbt runs via Airflow or Snowflake Tasks
- Build a Streamlit frontend for interactivity

---

## ğŸ™Œ Acknowledgements

Data sourced from the Tokyo 2020 public datasets. This project is for educational and portfolio purposes.



